{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import operator\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " 引子:\n",
    "    机器学习算法其实很古老，作为一个码农经常会不停的敲if, else if, else,其实就已经在用到决策树的思想了。\n",
    "    只是你有没有想过，有这么多条件，用哪个条件特征先做if，哪个条件特征后做if会比较优呢？\n",
    "    怎么准确的定量选择这个标准就是决策树机器学习算法的关键了。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.决策树算法原理（信息熵entropy）\n",
    "2.用Python实现决策树的构造\n",
    "3.测试算法：使用决策树执行分类\n",
    "4.使用算法：决策树的存储"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.决策树的算法原理：\n",
    "    (1)在构造决策树时，我们首先考虑的就是在当前数据集中的众多特征中，哪一个特征能够划分出最好的结果呢？\n",
    "    划分数据集的最大原则是：将无序的数据集变得更加有序。如何量化数据集的“序”，自然而然的就想起来香农熵或者简称熵\n",
    "    (2)熵度量了实物的不确定性，越不确定的实物，它的熵就越大；随机变量X的熵表达式H(X)如下：\n",
    "    H(X)=-sum(pi*log pi) 假设X共有n个类别，i的取值为1-n,pi表示X属于第i类的概率，最后计算累加和再求相反数；\n",
    "    函数实现为：calc_shannon_entropy()\n",
    "    每个数据集的信息熵是唯一确定的，只与样本的类别有关，与特征无关。\n",
    "    (3)在第二步的基础上，我们开始学习如何划分数据集，做法是对每个特征划分数据集，在计算划分后的信息熵，计算方法如下：\n",
    "    假如原始数据集共10条数据，特征A共有三种取值，按照特征A划分的话，得到三个子集，样本数分别是3、4、3，那么划分之后的信息熵为\n",
    "    3/10*H(A1) + 4/10*H(A2) + 3/10*H(A3)\n",
    "    (4)递归构造决策树：\n",
    "       递归过程：得到原始数据集，基于最好的属性划分数据集，由于特征值可能有多个，因此可能存在多个分支将数据集划分，对每个分支数据进行分割操作；\n",
    "       递归结束条件：\n",
    "            第一个停止条件是所有的类别标签完全相同，则直接返回该类别标签；\n",
    "            第二个停止条件是使用完了所有特征，仍然不能将数据划分仅包含唯一类别的分组，即决策树构建失败，特征不够用\n",
    "            此时说明数据维度不够，由于第二个停止条件无法简单地返回唯一的类标签，这里挑选出数量最多的类别作为返回值"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "测试算法 --- 使用决策树进行分类\n",
    "    采用递归思想 ，遍历tree(字典结构)的keys(),一直读到对应的value不再是dict结构的key为止，\n",
    "    函数实现见classify()\n",
    "\n",
    "使用算法 --- 决策树的存储\n",
    "    构造决策树是一个很耗时的任务，如果数据集很大，会耗费更多的计算时间，对应的，决策树执行分类任务时，时间很快。\n",
    "    因此，要做的是，存储构建好的决策树，从而提高效率。\n",
    "    我们需要使用python的pickle模块序列化对象，因为序列化对象可以保存在磁盘上，在需要的时候再读取出来，任何对象都可执行序列化操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个简单的数据集\n",
    "def create_data_set():\n",
    "    data_set = [[1, 1, 'yes'],\n",
    "                [1, 1, 'yes'],\n",
    "                [1, 0, 'no'],\n",
    "                [0, 1, 'no'],\n",
    "                [0, 1, 'no']]\n",
    "    labels = ['no surfacing', 'flippers']  # 0,1所对应的特征名\n",
    "    return data_set, labels\n",
    "data_set ,labels= create_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算给定数据集的香浓熵\n",
    "def calc_shannon_entropy(data_set):\n",
    "    num_enties = len(data_set)\n",
    "    label_count={}\n",
    "    for entry in data_set:\n",
    "        current_label = entry[-1] # 每一条记录的最后一个元素，为他的label\n",
    "        label_count[current_label] = label_count.get(current_label,0) +1 # {'yes': 2, 'no': 3}\n",
    "    shannon_entropy =0.0\n",
    "    for label in label_count:\n",
    "        prob = label_count[label] / num_enties\n",
    "        shannon_entropy += -prob*log(prob,2) \n",
    "    return shannon_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_set为原始数据集,axis为进行划分的维度（特征）,value为该维度的特征值,返回满足该特征值的所有列表的集合\n",
    "如：原始数据集为\n",
    "[[1, 1, 'yes'], \n",
    "[1, 1, 'yes'], \n",
    "[1, 0, 'no'], \n",
    "[0, 1, 'no'], \n",
    "[0, 1, 'no']]\n",
    "函数参数值为split_data_set(data, 0, 0) ; 返回:[[1, 'no'], [1, 'no']]\n",
    "\"\"\"\n",
    "def split_data_set(data_set,axis,value):\n",
    "    result_set = []\n",
    "    for entry in data_set:\n",
    "        if entry[axis] == value:\n",
    "            new_entry = entry[:axis] # 像extend，append，remove等方法，都是直接在列表上进行修改，返回为None 别用a=a.append(b)\n",
    "            new_entry.extend(entry[axis+1:])\n",
    "            result_set.append(new_entry)\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "choose_best_feat_to_split 函数思想\n",
    "    1.统计数据集中的特征数，逐个特征计算分裂后的信息增益；\n",
    "    2.计算信息增益过程如下：\n",
    "        a.统计特征A的取值(去重)\n",
    "        b.根据特征值样本分为几类，A=a1的样本为1类，A=a2的样本类为1类。。。\n",
    "        c.统计划分之后，每一类的样本数与总样本数的比值，再计算每一类的信息熵，最终加权求和\n",
    "    3.信息增益= 原始信息熵 - 分类后的信息熵，取最大新兴增益的那个特征作为分类特征\n",
    "\"\"\"\n",
    "def choose_best_feat_to_split(data_set):\n",
    "    num_of_feature = len(data_set[0])-1 #最后一列为label \n",
    "    base_entropy = calc_shannon_entropy(data_set) # 计算传进来的数据集的信息熵，而不是固定的原始数据集信息熵\n",
    "    best_info_gain =0.0\n",
    "    best_feature = -1\n",
    "    for i in range(num_of_feature):\n",
    "        feat_value_list = [entry[i] for entry in data_set] # 相当于是去除第i列的所有元素值，组成列表 ['yes', 'yes', 'no', 'no', 'no']\n",
    "        unique_feat_value = set(feat_value_list)\n",
    "        new_entropy=0.0\n",
    "        for feat_value in unique_feat_value:\n",
    "            sub_data_set = split_data_set(data_set,i,feat_value)\n",
    "            prob = len(sub_data_set)/float(len(data_set))\n",
    "            new_entropy += prob*calc_shannon_entropy(sub_data_set)\n",
    "        info_gain = base_entropy - new_entropy\n",
    "        if info_gain > best_info_gain:\n",
    "            best_info_gain = info_gain\n",
    "            best_feature = i\n",
    "    return best_feature                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数功能：统计 class_list中，出现次数最多的类别\n",
    "def majority_vote(class_list):\n",
    "    class_count={}\n",
    "    for element in class_list:\n",
    "        class_count[element] = class_count.get(element,0) + 1\n",
    "    sorted_class_count = sorted(class_count.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sorted_class_count[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_tree函数说明：\n",
    "# 输入示例：data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] ,\n",
    "#         labels = ['no surfacing', 'flippers']\n",
    "# 输出示例：{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
    "# 算法步骤：\n",
    "\"\"\"\n",
    "    1.提取传入的数据集中的所有类别元素，如果所有元素类别相同，直接返回该类别作为这个分支结束\n",
    "    2.如果传进来的数据集中，每一行只剩一个元素，即所有特征都划分完了，只剩下标签类，那么用多数投票原则，返回该最多的类别值，作为这个分支的结果\n",
    "    3.在两步没有返回的话，下面是递归的主要过程：\n",
    "        a.选择该数据集的最佳分割特征，找到该特征对应的特征名(主要是便于最终生成树)\n",
    "        b.创建树，字典结果，键名为最佳分割特征，键值为{}(递归到最后是类别名)\n",
    "        c.删除一分格的特征名，找到最佳分割特征所对应的特征值，将数据集分为几个部分，每个部分都不再宝行此特征字段。\n",
    "        d.递归的对分割的每一个数据集进行create_tree操作\n",
    "    4.返回my_tree 字典结构\n",
    "\"\"\"\n",
    "def create_tree(data_set,labels):\n",
    "    class_list = [entry[-1] for entry in data_set]\n",
    "    # 如果class_list中，第一个元素的个数等于总长度，说明该数据集集中，所有元素类别相同，则停止进行分类。\n",
    "    if class_list.count(class_list[0]) == len(class_list):\n",
    "        return class_list[0]\n",
    "    # len(data_set(0)) == 1 说明数据集只剩类标签，如[['yes'],['no'],['yes']] 没办法再分割数据集了\n",
    "    if len(data_set[0]) == 1:\n",
    "        return majority_vote(class_list)\n",
    "    best_feat = choose_best_feat_to_split(data_set)\n",
    "    best_feat_label = labels[best_feat]\n",
    "    my_tree = {best_feat_label:{}}\n",
    "    del labels[best_feat] # 删除特征名中的改元素，已经使用过不能再用\n",
    "    feat_values= [entry[best_feat] for entry in data_set]\n",
    "    unique_vals=set(feat_values)\n",
    "    for value in unique_vals:\n",
    "        sub_features = labels\n",
    "        my_tree[best_feat_label][value] = create_tree(split_data_set(data_set,best_feat,value),sub_features)\n",
    "    return my_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify函数说明：\n",
    "# 输入示例: input_tree : {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
    "#          feat_labels : ['no surfacing','flippers']\n",
    "#          test_vec : [1,1]\n",
    "# 输出示例: yes\n",
    "# 算法思想\n",
    "\"\"\"\n",
    "我们在创建决策树的时候，知道最终的类标签一定是在叶子节点上，用字典结果来说，就是最底层的value值，而就算是最简单的决策树。如：\n",
    "{'happy':{0:'no',1:'yes'}} 也要获取嵌套在里面的字典。\n",
    "所以classify实际上也就是一个递归函数，一直读到不再是dict结果的key为止\n",
    "\n",
    "步骤：\n",
    "    1.获取最外层的key，然后提取所对应的字典结构\n",
    "    2.找到上一步的key(特征名)所对应的索引值，便于我们在test_vec中找到对应的特征值\n",
    "    3.遍历第一步或得到的字典结构的keys，如果key对应的value仍是字典结构，则递归调用classify函数：\n",
    "\"\"\"\n",
    "def classify(input_tree, feat_labels, test_vec):\n",
    "    first_key = list(input_tree.keys())[0]  # 字典的key不支持索引，所以转成list形式\n",
    "    second_dict = input_tree[first_key]\n",
    "    feat_index = feat_labels.index(first_key)\n",
    "    for key in second_dict.keys():\n",
    "        if test_vec[feat_index] == key:\n",
    "            if type(second_dict[key]).__name__ == 'dict':\n",
    "                class_label = classify(second_dict[key], feat_labels, test_vec)\n",
    "            else:\n",
    "                class_label = second_dict[key]\n",
    "    return class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储决策树到硬盘\n",
    "def store_tree(input_tree, file_name):\n",
    "    # 这里的mode,要写作'wb'\n",
    "    fw = open(file_name, 'wb')\n",
    "    pickle.dump(input_tree, fw)\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从硬盘中加载决策树\n",
    "def load_tree(file_name):\n",
    "    fr = open(file_name, 'rb')\n",
    "    return pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
      "该物种是鱼类\n"
     ]
    }
   ],
   "source": [
    "data,feat_labels = create_data_set()\n",
    "# 小知识：python 中，\"a = b\"表示的是对象 a 引用对象 b，对象 a 本身没有单独分配内存空间(重要：不是复制！)\n",
    "# 这里对feat_labels进行拷贝操作，因为在create_tree的算法中，对feat_labels进行删除操作   \n",
    "backup = copy.copy(feat_labels)\n",
    "my_tree = create_tree(data, feat_labels)\n",
    "print(my_tree)\n",
    "store_tree(my_tree,'my_decision_tree.txt')\n",
    "result = classify(my_tree,backup,[1,1])\n",
    "if result == 'yes':\n",
    "    print(\"该物种是鱼类\")\n",
    "else:\n",
    "    print('该物种不是鱼类')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
