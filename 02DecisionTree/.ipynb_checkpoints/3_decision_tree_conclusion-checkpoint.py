# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 2/26/19 3:10 PM
 @desc: DecisionTree算法学习总结
"""

"""
 1.决策树分类器，可以看成是一个带有终止块的流程图，终止块表示分类结果。
 2.决策树的构建，是算法的核心内容，我们依据的数学理论是香农熵，按照哪一个特征进行分割数据集，使得信息增益最大，是最原始的决策树构建算法，
 也是我们熟知的ID3算法，此外，现在更加流行的还有C4.5、CART等算法，都是用于决策树的构造；
 3.代码实现决策树的构建，采用了递归的思想，递归终止条件是 分支上的数据集类标签全部相同 或者 所有的特征都已经用于分割数据集了
 4.代码实现决策树的预测，同样也是采用了递归的思想，递归的返回条件是key所对应的value不再是dict结构，即遇到叶子节点就返回
"""

"""
 决策树ID3算法的不足:
    1.ID3没有考虑连续型特征，比如长度、密度都是连续型，无法在ID3上运用;
    2.ID3采用信息增益大的特征作为建立决策树的节点，但是有人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大
      比如一个变量有两个值，各为1/2，另有一个变量有三个值，各为1/3，但是后者的信息熵比前者大，而分枝多的属性不一定是最优的选择;
    3.ID3对于缺失值的情况也没有做考虑
    4.ID3产生的树很容易产生过拟合
"""

# ID3((Iterative Dichotomiser 3,迭代二叉树三代)算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法，
# 也许你会问，为什么不叫ID4，ID5之类的名字呢?
# 那是因为决策树太火爆，他的ID3一出来，别人二次创新，很快 就占了ID4， ID5，所以他另辟蹊径，取名C4.0算法，
# 后来的进化版为C4.5算法，“第4.5代分类器”，后续的商业化版本称为C5.0

"""
 C4.5的改进: 针对上述的四个不足，分别作出改进，进化成C4.5 (推荐观看“西瓜书”《机器学习》周志华：P83 4.4连续值与缺失值章节)
    1.问题：不能处理连续型特征
        C4.5的思路是将连续的特征离散化，比如m个样本的连续特征A有n个，从小到大排列为a1,a2,a3,...,an，则C4.5取相邻两样本值的平均数，
        一共取得n-1个划分点，然后分别计算每一个划分点作为二元分类点时的信息增益比，选择信息增益比最大的点ai作为该连续特征的二元离散分类点，
        小于ai的值，为类别1，大于ai的值为类别2，这就是连续特征离散化的过程；
        然后该二元分类点和其他离散特征一起做比较，选择在当前数据集上的最优分割点；
        （注意：如果当前节点为连续属性，之后还可以参与子节点的产生选择过程）
    2.问题：信息增益作为标准容易偏向于取值较多的特征
        C4.5引入了信息增益比的概念，通过信息增益比来选择特征,信息增益比 = 信息增益/特征熵,【特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题】
        特征熵 = -sum(ratio(i) * log ratio(i)) （假设特征A共有n个取值,ratio(i)表示特征A的值为i的样本数与总样本数的比值,分别计算每个特征值的样本比例,最后累加再求相反数）
    3.问题：不能解决缺失值
        分解为两个小问题：
            （1）如何在属性值缺失的情况下进行划分属性的选择？
            （2）给定划分属性，如何划分该属性值缺失的样本属于的数据集？
        针对第一个小问题：
            C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征A的数据集D1，另一部分是没有特征A的数据集D2. 
            首先计算ρ，(ρ=D1的样本数/总样本数)， 然后计算数据集D1上特征A的信息熵H1（不是信息增益比），然后再计算出D1按照特征A的几个特征值划分之后，每一个
            数据子集的信息熵，计算数据子集的信息熵的加权和H2，权重 = (D1子集的样本数/D1样本数) * D1子集的信息熵
            # 因此，数据集D1，按照特征A的分割，信息增益为 H1-H2
            # 对于整个数据集而言，按照特征A的分割，信息增益为ρ*(H1-H2)
        针对第二个小问题：
            可以将缺失特征的样本同时划分入所有的子节点，不过需要分配不同的权重
            举个例子，划分属性为A,A对应三个特征值a1,a2,a3，样本个数分别为3,4,5，特征A值缺失的样本数共3个，那么前面未缺失的样本，分别进入
            对应的子集，权值为1，而剩余的三个缺失样本，每一个都分配到三个子集，进入每个子集的权值分别为3/12,4/12,5/12
    4.问题：过拟合
        C4.5引入了正则化系数进行初步的剪枝。具体方法这里不讨论。之后讲CART（分类和回归树）的时候会详细讨论剪枝的思路。
"""

"""
 C4.5算法的不足：
    1.由于决策树算法非常容易过拟合，因此对于生成的决策树必须进行剪枝操作。
        C4.5的剪枝方法有优化的空间，一种是预剪枝，在生成决策树的过程中，决定是否继续向下划分数据集，用多数投票法决定当前分支标签；
        第二种是后剪枝，先生成决策树，通过交叉验证法，如果剪枝后，验证集的精度提升，那么决定剪枝
    2.C4.5和ID3一样，生成的是多叉树，很多时候，在计算机中，二叉树模型会比多叉树运算效率高，如果采用二叉树，可以提升效率
    3.C4.5只能用于分类
    4.C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算
"""

# 针对以上四个问题，CART树都进行了改进，所以目前如果不考虑集成学习话，在普通的决策树算法里，CART算法算是比较优的算法了。
# scikit-learn的决策树使用的也是CART算法

"""
 CART算法主要的不足：
    1.无论是ID3,C4.5还是CART算法，在做特征选择时都是选择最优的一个特征来做分类决策，但是在大多数时候，分类决策不应该是由某一个特征决定的，
    而是应该由一组特征决定，这种决策树称为多变量决策树，在进行分类决策时，多变量决策树选择最优的一组特征线性组合来做决策，算法代表是OC1;
    2.如果样本发生一点点的改动，就会导致树结构的剧烈改变，通过随机森林可以解决此问题；
"""

"""
    决策树三种算法总结：
    ---------------------------------------------------------------------------------------------------
    | 算法         支持模型         树结构         特征选择            连续值处理       缺失值处理       剪枝  |
    ---------------------------------------------------------------------------------------------------
    | ID3           分类           多叉树        信息增益（熵差）        不支持          不支持       不支持  |
    | C4.5          分类           多叉树        信息增益比              支持            支持         支持  |
    | CART        分类与回归        二叉树       基尼系数，均方差          支持            支持         支持  |
    ---------------------------------------------------------------------------------------------------
    
    决策树的优点：
        1.简单直观，生成的决策树可以通过绘图直观显示；
       *2.基本不需要做预处理，不需要进行数值归一化（概率模型的算法，都不需要归一化），算法本身也会处理缺失值、连续值；
        3.使用决策树预测的代价是O(log2 m),m为样本数；
        4.即可以处理离散值，也可以处理连续值；
        5.相比于神经网络，决策树是白盒模型，在逻辑上可以得到很好的解释；
        6.对于异常点的容错能力好，健壮性高
    
    决策树的缺点：
        1.容易过拟合，有几种解决策略（设置叶子节点最小样本数、树的最大深度、正则化系数等）；
        2.决策树很容易因为少量样本分布的改变而导致树结构剧烈的改变，随机森林可以解决此问题；
        3.寻找最优的决策树是一个NP难问题，我们用的是贪心算法，很容易陷入局部最优，通过集成学习之类的方法来改善；
        4.有些复杂的关系，比如异或，决策树很难学习，用神经网络之类的算法来解决；
        5.如果某些特征的样本比例过大，生成的决策树容易偏向于这些特征，通过调节样本权重来改善；
"""
